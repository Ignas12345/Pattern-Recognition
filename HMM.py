import numpy as np
from DiscreteD import DiscreteD
from GaussD import GaussD
#from MarkovChain import MarkovChain


class HMM:
    """
    HMM - class for Hidden Markov Models, representing
    statistical properties of random sequences.
    Each sample in the sequence is a scalar or vector, with fixed DataSize.
    
    Several HMM objects may be collected in a single multidimensional array.
    
    A HMM represents a random sequence(X1,X2,....Xt,...),
    where each element Xt can be a scalar or column vector.
    The statistical dependence along the (time) sequence is described
    entirely by a discrete Markov chain.
    
    A HMM consists of two sub-objects:
    1: a State Sequence Generator of type MarkovChain
    2: an array of output probability distributions, one for each state
    
    All states must have the same class of output distribution,
    such as GaussD, GaussMixD, or DiscreteD, etc.,
    and the set of distributions is represented by an object array of that class,
    although this is NOT required by general HMM theory.
    
    All output distributions must have identical DataSize property values.
    
    Any HMM output sequence X(t) is determined by a hidden state sequence S(t)
    generated by an internal Markov chain.
    
    The array of output probability distributions, with one element for each state,
    determines the conditional probability (density) P[X(t) | S(t)].
    Given S(t), each X(t) is independent of all other X(:).
    
    
    References:
    Leijon, A. (20xx) Pattern Recognition. KTH, Stockholm.
    Rabiner, L. R. (1989) A tutorial on hidden Markov models
    	and selected applications in speech recognition.
    	Proc IEEE 77, 257-286.
    
    """
    def __init__(self, mc, distributions):

        self.stateGen = mc
        self.outputDistr = distributions

        self.nStates = mc.nStates
        self.dataSize = distributions[0].dataSize
    
    def rand(self, nSamples):
        """
        [X,S]=rand(self,nSamples); generates a random sequence of data
        from a given Hidden Markov Model.
        
        Input:
        nSamples=  maximum no of output samples (scalars or column vectors)
        
        Result:
        X= matrix or row vector with output data samples
        S= row vector with corresponding integer state values
          obtained from the self.StateGen component.
          nS= length(S) == size(X,2)= number of output samples.
          If the StateGen can generate infinite-duration sequences,
              nS == nSamples
          If the StateGen is a finite-duration MarkovChain,
              nS <= nSamples
        """
        S = self.stateGen.rand(nSamples)
        #For scalar observations
        if self.outputDistr[0].dataSize == 1:
            X = np.zeros(len(S))
        #For vector observations
        else:
            #X = np.zeros((self.outputDistr[0].dataSize, len(S)))
            X = np.zeros((len(S), self.outputDistr[0].dataSize))
        
        #Generate observations for each state in the sequence
        for i, state in enumerate(S):
            #Select the distribution corresponding to the current state
            distr = self.outputDistr[state - 1]  #Adjusting index for 0-based Python indexing
            
            #Generate a random observation from the distribution
            """print("X_shape ", X.shape)
            print("rand1 ", distr.rand(1))
            print("rand2 ",distr.rand(2))"""
            X[i] = distr.rand(1).flatten() 
        return X, S
        
    def viterbi(self):
        pass

    def train(self):
        pass

    def stateEntropyRate(self):
        pass

    def setStationary(self):
        pass

    def logprob(self, pX, scale_factors):
        alpha, c = self.stateGen.forward(pX)
        logprob = np.sum(np.log(c))
        logP = logprob + np.sum(np.log(scale_factors))
        return logP

    def adaptStart(self):
        pass

    def adaptSet(self):
        pass

    def adaptAccum(self):
        pass
    def updateq(self, alpha, beta, c):
        """alpha Forward probabilities, shape (N, T) where N is number of states, T is number of time steps
        beta  Backward probabilities, shape (N, T)
        c Scaling factors for each time step, shape (T,)"""
        alpha_0 = alpha[:, 0]  # Forward probabilities for all states at time 1
        beta_0 = beta[:, 0]    # Backward probabilities for all states at time 1
        c_0 = c[0]             # Scaling factor at the first time step

        # Calculate the updated initial probabilities
        new_q = (alpha_0 * beta_0) / c_0

        # Normalize new_q to ensure it sums to 1
        new_q = (new_q / np.sum(new_q))
        
        # Update the model's initial state probabilities
        self.stateGen.q = new_q

    def updateA(self, alpha, beta, emission_matrix):
        if self.stateGen.B is None:
            emission_matrix = emission_matrix
        else:
            emission_matrix = self.stateGen.B
        n_states = alpha.shape[0]
        T = alpha.shape[1]
        A_new = np.zeros((n_states, n_states))
        xi = np.zeros((n_states, n_states, T-1))
        for i in range(n_states):
            for j in range(n_states):
                for t in range(T - 1):
                    #scaling_factor = emission_matrix[:, t].max()
                    #sannolikheten att fÃ¥ x_{t+1} men b_j
                    #here i use the scaling factor to get the true probability
                    #xi[i, j ,t] = alpha[i, t] * self.stateGen.A[i, j] * emission_matrix[j][t+1] * scaling_factor * beta[j, t + 1]
                    xi[i, j ,t] = alpha[i, t] * self.stateGen.A[i, j] * emission_matrix[j][t+1] *  beta[j, t + 1]
        #get xi_{i,j}
        xi_sum = np.sum(xi, axis=2)
        for i in range(n_states):
            total_transitions_from_i = np.sum(xi_sum[i, :])  # Sum over all j for state i
            for j in range(n_states):
                A_new[i, j] = xi_sum[i, j] / total_transitions_from_i
        self.stateGen.A = A_new 
    
    def updateB(self, alpha, beta, c, observations):
        epsilon = 1e-12
        N, T = alpha.shape
        new_means = np.zeros(N)
        new_vars = np.zeros(N)
        gammas = np.zeros((N, T))
        """for j in range(N):
            for t in range(T):
                gammas[j,t] = alpha[j, t] * beta[j, t] / (c[t] + epsilon)
        # Update Gaussian parameters for each state
        for i in range(N):
            weighted_observations = gammas[i, :] * observations
            sum_gammas = np.sum(gammas[i, :])

            new_means[i] = np.sum(weighted_observations) / sum_gammas 

            mean_adjusted_squares = (observations - new_means[i]) ** 2
            new_vars[i] = np.sum(gammas[i, :] * mean_adjusted_squares) / sum_gammas"""
        for t in range(T):
            norm_factor = np.sum(alpha[:, t] * beta[:, t]) + epsilon  # Normalization factor at time t
            for j in range(N):
                gammas[j, t] = alpha[j, t] * beta[j, t] / norm_factor
        
            # Update Gaussian parameters for each state using loops
        for i in range(N):
            weighted_observations = np.zeros(T)
            sum_gammas = 0

                # Calculate weighted observations and sum of gammas for state i
            for t in range(T):
                weighted_observations[t] = gammas[i, t] * observations[t]
                sum_gammas += gammas[i, t]
            
            new_means[i] = np.sum(weighted_observations) / sum_gammas
        

            # Compute the new variances for state i
            mean_adjusted_squares = np.zeros(T)
            for t in range(T):
                mean_adjusted_squares[t] = (observations[t] - new_means[i]) ** 2 * gammas[i, t]

            
            new_vars[i] = np.sum(mean_adjusted_squares) / sum_gammas
        pX = np.zeros((N, T))
        scale_factors = np.zeros(T)
        for k in range(N):
            distribution = GaussD(means=[new_means[k]], stdevs=[np.sqrt(new_vars[k])])
            print("means", new_means[k])
            print("std", np.sqrt(new_vars[k]))
            for t in range(T):
                pX[k, t] = distribution.prob(observations[t])
            #scale_factors[t] = pX[:, t].max()
            #pX[:, t] /= scale_factors[t]
        self.stateGen.B = pX

    def train(self, observations, initial_pX, num_iterations=4):
        for iteration in range(num_iterations):
            #The code is if self.stateGen.B == None use pX else use B
            alpha_hat, scale_factors = self.stateGen.forward(initial_pX)
            beta_hat = self.stateGen.backward(scale_factors, initial_pX)

            self.updateA(alpha_hat, beta_hat, initial_pX)
            print("A-matrix", self.stateGen.A)
            self.updateB(alpha_hat, beta_hat, scale_factors, observations)
            print("B-matrix", self.stateGen.B)
            print(f"Iteration {iteration + 1} complete")

    def viterbi(self, obs):
        """
        Viterbi algorithm to find the most probable state sequence given the observation sequence.
        
        Parameters:
            obs (list or array): Sequence of observations.
        
        Returns:
            list: The most probable sequence of states.
        """
        N = self.nStates  # Number of states
        T = len(obs)  # Total number of observations
        A = self.stateGen.A
        pi = self.stateGen.q
        
        # Log probabilities for numerical stability
        logA = np.log(A + 1e-12)
        logB = np.zeros((N, T))
        for i in range(N):
            for t in range(T):
                logB[i, t] = np.log(self.outputDistr[i].prob(obs[t]) + 1e-12)

        logPi = np.log(pi + 1e-12)

        # Initialize the DP arrays
        V = np.zeros((N, T))
        path = np.zeros((N, T), dtype=int)

        # Initialize base cases (t == 0)
        V[:, 0] = logPi + logB[:, 0]

        # Run Viterbi for t > 0
        for t in range(1, T):
            for j in range(N):
                seq_probs = V[:, t-1] + logA[:, j] + logB[j, t]
                V[j, t] = np.max(seq_probs)
                path[j, t] = np.argmax(seq_probs)

        # Backtrack
        states = np.zeros(T, dtype=int)
        states[-1] = np.argmax(V[:, -1])
        for t in range(T-2, -1, -1):
            states[t] = path[states[t+1], t+1]

        return states.tolist()

    def classify_sequence(self, state_sequence):
        #classify the sequence based on the most likely state sequence. classify the sequence as the most common state in the estimated sequence
        standing_counter = 0
        walking_counter = 0
        running_counter = 0
        for state in state_sequence:
            if state == 0:
                standing_counter += 1
            elif state == 1:
                walking_counter += 1
            elif state == 2:
                running_counter += 1
        max_count = max(standing_counter, walking_counter, running_counter)
        if max_count == standing_counter:
            print("The sequence is classified as standing!")
        elif max_count == walking_counter:
            print("The sequence is classified as walking!")
        else:
            print("The sequence is classified as running!")
        


    